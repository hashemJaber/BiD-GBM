{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install omegaconf\n",
    "!pip install  pyntcloud\n",
    "!pip install open3d\n",
    "!pip install OpenCV\n",
    "!pip install Plotly\n",
    "!pip install psutil requests\n",
    "!apt install aria2\n",
    "!apt-get install -y orca\n",
    "import tarfile\n",
    "import os\n",
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import Button, VBox\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Conv2D, BatchNormalization, Activation, UpSampling2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import pyarrow.feather as feather\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "##### Note: This will taske a while, please use aria2c as this speeds up the download read more: add_Me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for train and test files if they don't exist\n",
    "!mkdir -p /argotrain\n",
    "#!mkdir -p /argotest\n",
    "\n",
    "# Train 1\n",
    "!aria2c -x 16 https://s3.amazonaws.com/argoverse/datasets/av2/tars/sensor/train-000.tar -d /argotrain -o train-000.tar\n",
    "\n",
    "# Test 1\n",
    "!aria2c -x 16 https://s3.amazonaws.com/argoverse/datasets/av2/tars/sensor/test-000.tar -d /argotest -o test-000.tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To visualize lidar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "lidar_file = '/argotrain/sensor/train/0749e9e0-ca52-3546-b324-d704138b11b5/sensors/lidar/315972769159647000.feather'\n",
    "\n",
    "\n",
    "lidar_df = feather.read_feather(lidar_file)\n",
    "\n",
    "\n",
    "lidar_data = lidar_df.to_numpy()\n",
    "\n",
    "# Prepare your point cloud data\n",
    "x = lidar_data[:, 0]\n",
    "y = lidar_data[:, 1]\n",
    "z = lidar_data[:, 2]\n",
    "\n",
    "\n",
    "intensity = lidar_data[:, 3] if lidar_data.shape[1] > 3 else np.ones_like(x)\n",
    "camera = dict(\n",
    "    eye=dict(x=0.980957, y=-4.300781, z=1.241211)  # Position the camera here (x, y, z are the distances)\n",
    ")\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=x, y=y, z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=1,\n",
    "        color=intensity,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Set plot details\n",
    "fig.update_layout(scene = dict(\n",
    "                    xaxis_title='X',\n",
    "                    yaxis_title='Y',\n",
    "                    zaxis_title='Z'),\n",
    "                    width=700,\n",
    "                    margin=dict(r=0, l=0, b=0, t=0),\n",
    "                    meta=camera)\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"/contentlidar_plot.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To visualize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tarfile\n",
    "import os\n",
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import Button, VBox\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "\n",
    "# place holder.\n",
    "image_file=  '/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/ring_front_center/315967378149927216.jpg'\n",
    "\n",
    "image = mpimg.imread(image_file)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# helper function if you want to just visualize images via file path\n",
    "def plot_lidar_and_image(feather_file:str):\n",
    "    try:\n",
    "        # Extract timestamp from feather file name\n",
    "        match = re.search(r\"(\\d+)\\.feather\", feather_file)\n",
    "        if not match:\n",
    "            print(\"Could not extract timestamp from feather file name.\")\n",
    "            return\n",
    "\n",
    "        lidar_timestamp = int(match.group(1))\n",
    "\n",
    "        lidar_df = feather.read_feather(feather_file)\n",
    "        lidar_data = lidar_df.to_numpy()\n",
    "\n",
    "        # ... (your existing lidar plotting code)\n",
    "\n",
    "        # Find the nearest image\n",
    "        image_dir = os.path.dirname(feather_file).replace(\"sensors/lidar\", \"sensors/cameras/ring_front_center\")\n",
    "        image_files = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "\n",
    "        min_diff = float('inf')\n",
    "        closest_image = None\n",
    "\n",
    "        for image_file in image_files:\n",
    "            image_match = re.search(r\"(\\d+)\\.jpg\", image_file)\n",
    "            if image_match:\n",
    "                image_timestamp = int(image_match.group(1))\n",
    "                diff = abs(image_timestamp - lidar_timestamp)\n",
    "                if diff < min_diff:\n",
    "                    min_diff = diff\n",
    "                    closest_image = os.path.join(image_dir, image_file)\n",
    "\n",
    "        if closest_image:\n",
    "          print(closest_image)\n",
    "          image = mpimg.imread(closest_image)\n",
    "          plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
    "          plt.imshow(image)\n",
    "          plt.axis('off')  # Hide axes\n",
    "          plt.title(f\"Nearest Image to {feather_file}\")  # Display file name\n",
    "          plt.show()\n",
    "\n",
    "        else:\n",
    "            print(\"No corresponding image file found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "# Example usage (replace with your actual file path):\n",
    "plot_lidar_and_image('/argotrain/sensor/train/0749e9e0-ca52-3546-b324-d704138b11b5/sensors/lidar/315972769159647000.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "class ArgoverseDataset_Images_To_Lidar(Dataset):\n",
    "    def __init__(self, root_dir:str, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.lidar_files = []\n",
    "        for subdir, _, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".feather\"):\n",
    "                    self.lidar_files.append(os.path.join(subdir, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lidar_files)\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        lidar_file = self.lidar_files[idx]\n",
    "\n",
    "        try:\n",
    "            # Extract timestamp from feather file name\n",
    "            match = re.search(r\"(\\d+)\\.feather\", lidar_file)\n",
    "            if not match:\n",
    "                print(f\"Could not extract timestamp from {lidar_file}. Skipping.\")\n",
    "                return None  # Skip this item if timestamp extraction fails\n",
    "\n",
    "            lidar_timestamp = int(match.group(1))\n",
    "\n",
    "            lidar_df = feather.read_feather(lidar_file)\n",
    "            lidar_data = lidar_df.to_numpy()\n",
    "\n",
    "            # Normalize lidar data (example)\n",
    "            lidar_data = lidar_data[:,0:3]\n",
    "            lidar_data = lidar_data / np.max(np.abs(lidar_data)) # Normalize to [-1, 1]\n",
    "            lidar_data = torch.tensor(lidar_data, dtype=torch.float32)\n",
    "            # example of explanations\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/lidar/315967376859506000.feather\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/ring_front_center\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/ring_front_left\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/ring_front_right\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/ring_rear_left\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/ring_rear_right\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/ring_side_left\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/stereo_front_left\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/stereo_front_right\n",
    "            #/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/cameras/ring_front_center/315967376899927209.jpg\n",
    "\n",
    "            # Load surrounding images\n",
    "            images = []\n",
    "            base_dir = os.path.dirname(lidar_file)\n",
    "            camera_types = {\n",
    "                \"front\": \"ring_front_center\",\n",
    "                \"left\": \"ring_front_left\",\n",
    "                \"right\": \"ring_front_right\",\n",
    "                \"rear_left\":\"ring_rear_left\",\n",
    "                \"rear_right\":\"ring_rear_right\",\n",
    "                \"side_left\":\"ring_side_left\",\n",
    "                \"side_right\":\"ring_side_right\",\n",
    "                \"stereo_front_left\":\"stereo_front_left\",\n",
    "                \"stereo_front_right\":\"stereo_front_right\"\n",
    "                # Add more cameras as needed\n",
    "            }\n",
    "\n",
    "            for cam_name, cam_type in camera_types.items():\n",
    "                image_dir = base_dir.replace(\"sensors/lidar\", f\"sensors/cameras/{cam_type}\")\n",
    "                min_diff = float('inf')\n",
    "                closest_image = None\n",
    "\n",
    "                for image_file in os.listdir(image_dir):\n",
    "                    if image_file.endswith(\".jpg\"):\n",
    "                        image_match = re.search(r\"(\\d+)\\.jpg\", image_file)\n",
    "                        if image_match:\n",
    "                            image_timestamp = int(image_match.group(1))\n",
    "                            diff = abs(image_timestamp - lidar_timestamp)\n",
    "                            if diff < min_diff:\n",
    "                                min_diff = diff\n",
    "                                closest_image = os.path.join(image_dir, image_file)\n",
    "\n",
    "                if closest_image:\n",
    "                    image = Image.open(closest_image).convert(\"RGB\")\n",
    "                    if self.transform:\n",
    "                        image = self.transform(image)\n",
    "                    image=image[0:1,:,:]\n",
    "                    images.append(image)\n",
    "                else:\n",
    "                    print(f\"Missing image for camera {cam_name} near {lidar_file}\")\n",
    "\n",
    "                    images.append(torch.zeros(3, 224, 224))  # Example placeholder, in case of error, just for safety\n",
    "\n",
    "            print(\"lidar_data\", lidar_data.shape)\n",
    "            print(\"images\", torch.stack(images).shape)\n",
    "\n",
    "            # Return lidar data and images as tensors\n",
    "            return lidar_data, torch.stack(images)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {lidar_file}: {e}\")\n",
    "            return None\n",
    "# Assuming we've defined our transformations (e.g., using torchvision.transforms) for y labels and vice versa\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images\n",
    "    transforms.ToTensor(),           # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #Normalize\n",
    "])\n",
    "\n",
    "\n",
    "dataset = ArgoverseDataset('/argotrain/sensor/train', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: [i for i in x if i is not None])\n",
    "\n",
    "for batch in dataloader:\n",
    "    if batch: # Check if batch is not empty\n",
    "        # Access lidar_data and images from the inner tuples\n",
    "        lidar_data, images = batch[0][0], batch[0][1]\n",
    "\n",
    "        print(\"Lidar data shape:\", lidar_data.shape)\n",
    "        print(\"Images shape:\", images.shape)\n",
    "    \n",
    "    break # Break after our first batch for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For our lidar generator via images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LidarGenerator(nn.Module):\n",
    "    def __init__(self, num_images:int=9, image_channels:int=1):\n",
    "        super(LidarGenerator, self).__init__()\n",
    "        self.num_images = num_images\n",
    "        self.image_channels = image_channels\n",
    "\n",
    "        # Image Conv is our siamese network\n",
    "        self.image_conv = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.SELU(),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        #self.attention= nn.Transformer(d_model=512, nhead=4,6,) FIXME\n",
    "        # Lidar generation layers\n",
    "        self.lidar_fc = nn.linear(256 * 28 * 28, 10500*3) #remember our X,Y,Z plus padding\n",
    "    def forward(self, images):  # Add latent_vec to the input arguments\n",
    "\n",
    "        # Process images\n",
    "        embedings=[]\n",
    "        for image in images:\n",
    "          image_features = self.image_conv(image)\n",
    "          embedings.append(embedings)\n",
    "\n",
    "        # Generate lidar points\n",
    "        lidar_points = self.lidar_fc(combined)\n",
    "        return lidar_points\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LidarGenerator().to(device)\n",
    "\n",
    "\n",
    "#generated_lidar = model(images.to(device))\n",
    "\n",
    "#print(\"Generated lidar shape:\", generated_lidar.shape) FIXME\n",
    "\n",
    "criterion = nn.MSELoss()  # THIS LOSS IS SUBJECT TO CHANGE\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for lidar_data, images in dataloader:\n",
    "      if lidar_data is None or images is None:\n",
    "        continue\n",
    "\n",
    "      lidar_data, images = lidar_data.to(device), images.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      generated_lidar = model(images)\n",
    "      loss = criterion(generated_lidar, lidar_data)\n",
    "      loss.backward()\n",
    "      # Update the weights\n",
    "      optimizer.step()\n",
    "\n",
    "      print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting our training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(len(os.listdir('/argotrain/sensor/train/00a6ffc1-6ce9-3bc3-a060-6006e9893a1a/sensors/lidar')))\n",
    "\n",
    "print(len((os.listdir('/argotrain/sensor/train/'))))\n",
    "list_of_examples=os.listdir('/argotrain/sensor/train/')\n",
    "lidar_file_paths=[]\n",
    "for path in list_of_examples:\n",
    "  path='/argotrain/sensor/train/'+path+'/sensors/lidar'\n",
    "  print('lidar examples count',len(os.listdir(path)))\n",
    "  files = os.listdir(path)\n",
    "  for file in files:\n",
    "        lidar_file_paths.append(os.path.join(path, file))\n",
    "\n",
    "lidar_file_paths=sorted(lidar_file_paths)\n",
    "ring_front_center_file_paths=[]\n",
    "for path in list_of_examples:\n",
    "    path='/argotrain/sensor/train/'+path+'/sensors/cameras/ring_front_center'\n",
    "    print('camera examples count',len(os.listdir(path)))\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        ring_front_center_file_paths.append(os.path.join(path, file))\n",
    "\n",
    "ring_front_center_file_paths=sorted(ring_front_center_file_paths)\n",
    "\n",
    "\n",
    "\n",
    "ring_rear_right_file_paths = []\n",
    "for path in list_of_examples:\n",
    "    path='/argotrain/sensor/train/'+path+'/sensors/cameras/ring_rear_right'\n",
    "    print('camera examples count',len(os.listdir(path)))\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        ring_rear_right_file_paths.append(os.path.join(path, file))\n",
    "ring_rear_right_file_paths=sorted(ring_rear_right_file_paths)\n",
    "\n",
    "\n",
    "ring_side_right_file_paths = []\n",
    "for path in list_of_examples:\n",
    "    path='/argotrain/sensor/train/'+path+'/sensors/cameras/ring_side_right'\n",
    "    print('camera examples count',len(os.listdir(path)))\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        ring_side_right_file_paths.append(os.path.join(path, file))\n",
    "\n",
    "ring_side_right_file_paths=sorted(ring_side_right_file_paths)\n",
    "ring_side_left_file_paths=[]\n",
    "for path in list_of_examples:\n",
    "    path='/argotrain/sensor/train/'+path+'/sensors/cameras/ring_side_left'\n",
    "    print('camera examples count',len(os.listdir(path)))\n",
    "    # Concatenate the directory path with each file name and add it to the list\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        ring_side_left_file_paths.append(os.path.join(path, file))\n",
    "\n",
    "\n",
    "ring_side_left_file_paths=sorted(ring_side_left_file_paths)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_avg_points(lidar_file_paths:str):\n",
    "    # Function to determine the maximum number of points in the LiDAR datasets\n",
    "    max_points = 0\n",
    "    avg=0\n",
    "\n",
    "    for path in lidar_file_paths:\n",
    "        lidar_data = get_file_data(path, islidar=True)\n",
    "        avg=avg+lidar_data.shape[0]\n",
    "        if lidar_data.shape[0] > max_points:\n",
    "            max_points = lidar_data.shape[0]\n",
    "    return max_points,avg/len(lidar_file_paths)\n",
    "\n",
    "\n",
    "def pad_lidar_data(lidar_data:Tensor, max_points:int):\n",
    "    # Function to pad the LiDAR data to the maximum number of points\n",
    "    padding_size = max_points - lidar_data.shape[0]\n",
    "    if padding_size > 0:\n",
    "        padding = np.zeros((padding_size, lidar_data.shape[1]))\n",
    "        lidar_data = np.vstack([lidar_data, padding])\n",
    "    return lidar_data\n",
    "\n",
    "\n",
    "\n",
    "def truncate_lidar_data(lidar_data:Tensor, max_points:int):\n",
    "    if lidar_data.shape[0] > max_points:\n",
    "        lidar_data = lidar_data[:max_points, :]\n",
    "    return lidar_data\n",
    "\n",
    "\n",
    "\n",
    "def get_file_data(file_path:str, islidar=False):\n",
    "    if islidar:\n",
    "        return feather.read_feather(file_path).to_numpy()\n",
    "    else:\n",
    "        image = np.asarray(Image.open(file_path))\n",
    "        #Since front camera is different\n",
    "        if image.shape[0] == 2048 and image.shape[1] == 1550:\n",
    "            image = np.transpose(image, (1, 0, 2))\n",
    "        return image\n",
    "\n",
    "\n",
    "def visualize_image(file_path:str):\n",
    "    image = Image.open(file_path)\n",
    "    pyplot.imshow(image)\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "def visualize_lidar(file:str):\n",
    "\n",
    "    file_path = file\n",
    "    lidar_df = feather.read_feather(file_path)\n",
    "    lidar_data = lidar_df.to_numpy()\n",
    "\n",
    "\n",
    "    x = lidar_data[:, 0]\n",
    "    y = lidar_data[:, 1]\n",
    "    z = lidar_data[:, 2]\n",
    "    intensity = lidar_data[:, 3] if lidar_data.shape[1] > 3 else np.ones_like(x)\n",
    "\n",
    "\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=x, y=y, z=z,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color=intensity, \n",
    "            colorscale='Viridis',\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )])\n",
    "\n",
    "\n",
    "    fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n",
    "                      width=700, height=500,\n",
    "                      margin=dict(r=20, l=10, b=10, t=10))\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "def visualize_lidar_data(lidar_data:Tensor):\n",
    "    x = lidar_data[:, 0]\n",
    "    y = lidar_data[:, 1]\n",
    "    z = lidar_data[:, 2]\n",
    "    intensity = lidar_data[:, 3] if lidar_data.shape[1] > 3 else np.ones_like(x)\n",
    "\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=x, y=y, z=z,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=1,\n",
    "            color=intensity,  # Color by intensity or z\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )])\n",
    "\n",
    "\n",
    "    fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n",
    "                      width=700, height=500,\n",
    "                      margin=dict(r=20, l=10, b=10, t=10))\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def flatten_y_train(Y_train:bool):\n",
    "    batch_size = Y_train.shape[0]\n",
    "\n",
    "    Y_train_flattened = Y_train.reshape(batch_size, -1)\n",
    "    return Y_train_flattened\n",
    "\n",
    "def flatten_x_train(X_train:Tensor):\n",
    "    batch_size = X_train.shape[0]\n",
    "    X_train_flattened = X_train.reshape(batch_size, -1)\n",
    "    return X_train_flattened\n",
    "max_cloud_ppoints,avg_cloud_ppoints=get_max_avg_points(lidar_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidar to images data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    target_count_for_test=0\n",
    "\n",
    "    # Iterate over target indices (twice the number of lidar file paths)\n",
    "    for target_index in range(int((len(lidar_file_paths) * 2) * 0.009)): #reason why I have this is that we can use sampling techniques to better short cut the heave number of data usage\n",
    "        target_index=target_index*100\n",
    "        # Fetch the camera file paths for the target index\n",
    "        ring_rear_right_path = ring_rear_right_file_paths[target_index]\n",
    "        ring_side_right_path = ring_side_right_file_paths[target_index]\n",
    "        ring_side_left_path = ring_side_left_file_paths[target_index]\n",
    "        ring_front_center_path = ring_front_center_file_paths[target_index]\n",
    "\n",
    "        # Fetch the lidar file (one lidar file for every two camera files)\n",
    "        lidar_index = int(target_index / 2)\n",
    "        lidar_df = get_file_data(lidar_file_paths[lidar_index], islidar=True)\n",
    "        if(max_cloud_ppoints>lidar_df.shape[0]):\n",
    "          lidar_df=pad_lidar_data(lidar_df,max_cloud_ppoints)\n",
    "        if(max_cloud_ppoints<lidar_df.shape[0]):\n",
    "          lidar_df=truncate_lidar_data(lidar_df,max_cloud_ppoints)\n",
    "        lidar_df = lidar_df[:, :3]  # Only take the first 3 columns (x, y, z)\n",
    "        # Fetching camera data for each target view\n",
    "        ring_rear_right_df = get_file_data(ring_rear_right_path)\n",
    "        ring_side_right_df = get_file_data(ring_side_right_path)\n",
    "        ring_side_left_df = get_file_data(ring_side_left_path)\n",
    "        ring_front_center_df = get_file_data(ring_front_center_path)\n",
    "        # Defining flag arrays for the views\n",
    "        flag_rear_right = np.array([0, 0, 0, 1])\n",
    "        flag_side_right = np.array([0, 0, 1, 0])\n",
    "        flag_side_left = np.array([0, 1, 0, 0])\n",
    "        flag_front_center = np.array([1, 0, 0, 0])\n",
    "\n",
    "\n",
    "        # Create input data by appending LiDAR data with the corresponding flag, we are using multi-task learning hence why this is done, there is a way of this redundency but havent gotten my way around it yet\n",
    "        input_rear_right = np.hstack([lidar_df, np.tile(flag_rear_right, (lidar_df.shape[0], 1))])\n",
    "        input_side_right = np.hstack([lidar_df, np.tile(flag_side_right, (lidar_df.shape[0], 1))])\n",
    "        input_side_left = np.hstack([lidar_df, np.tile(flag_side_left, (lidar_df.shape[0], 1))])\n",
    "        input_front_center = np.hstack([lidar_df, np.tile(flag_front_center, (lidar_df.shape[0], 1))])\n",
    "        #print('index ', target_index)\n",
    "        #print('input_rear_right',input_rear_right.shape)\n",
    "        #print('input_side_right',input_side_right.shape)\n",
    "\n",
    "        #print('input_side_left',input_side_left.shape)\n",
    "        #print('input_front_center',input_front_center.shape)\n",
    "\n",
    "        # Checking and making sure that the camera data has the same shape before appending\n",
    "        if ring_rear_right_df.shape == ring_side_right_df.shape == ring_side_left_df.shape == ring_front_center_df.shape:\n",
    "            if target_count_for_test<=int((len(lidar_file_paths) * 2) * 0.009 *0.8*4 ):\n",
    "              X_train.append(input_rear_right)\n",
    "              Y_train.append(ring_rear_right_df)\n",
    "              target_count_for_test=target_count_for_test+1\n",
    "            else:\n",
    "              X_test.append(input_rear_right)\n",
    "              Y_test.append(ring_rear_right_df)\n",
    "              target_count_for_test=target_count_for_test+1\n",
    "            if(target_count_for_test<int((len(lidar_file_paths) * 2) * 0.009 *0.8 *4)):\n",
    "                X_train.append(input_side_right)\n",
    "                Y_train.append(ring_side_right_df)\n",
    "                target_count_for_test=target_count_for_test+1\n",
    "            else:\n",
    "                X_test.append(input_side_right)\n",
    "                Y_test.append(ring_side_right_df)\n",
    "                target_count_for_test=target_count_for_test+1\n",
    "            if(target_count_for_test<int((len(lidar_file_paths) * 2) * 0.009 *0.8*4 )):\n",
    "                X_train.append(input_side_left)\n",
    "                Y_train.append(ring_side_left_df)\n",
    "                target_count_for_test=target_count_for_test+1\n",
    "            else:\n",
    "                X_test.append(input_side_left)\n",
    "                Y_test.append(ring_side_left_df)\n",
    "                target_count_for_test=target_count_for_test+1\n",
    "\n",
    "            if(target_count_for_test<int((len(lidar_file_paths) * 2) * 0.009 *0.8*4)):\n",
    "                X_train.append(input_front_center)\n",
    "                Y_train.append(ring_front_center_df)\n",
    "                target_count_for_test=target_count_for_test+1\n",
    "            else:\n",
    "                X_test.append(input_front_center)\n",
    "                Y_test.append(ring_front_center_df)\n",
    "                target_count_for_test=target_count_for_test+1\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"Shape mismatch at index {target_index}. Skipping this set.\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        X_train = np.asarray(X_train)\n",
    "        Y_train = np.asarray(Y_train)\n",
    "        X_test = np.asarray(X_test)\n",
    "        Y_test = np.asarray(Y_test)\n",
    "        print(f\"Generated and saved X_train and Y_train with shapes {X_train.shape} and {Y_train.shape}\")\n",
    "        print(f\"Generated and saved X_test and Y_test with shapes {X_test.shape} and {Y_test.shape}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during conversion to numpy arrays: {e}\")\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Reshape, Conv2DTranspose, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "def custom_flattened_mse_loss(y_true:Tensor, y_pred:Tensor):\n",
    "        #tf.print(\"applying loss\")\n",
    "\n",
    "        # Flatten the ground truth (y_true) and predicted (y_pred) tensors\n",
    "        tf.print(y_true.shape)\n",
    "        tf.print(y_pred.shape)\n",
    "        y_true_flat = K.flatten(y_true)\n",
    "        tf.print(y_true_flat.shape)\n",
    "        #y_pred_flat = K.flatten(y_pred)\n",
    "\n",
    "\n",
    "        mse_loss = K.mean(K.square(y_true_flat - y_pred))\n",
    "\n",
    "\n",
    "        return mse_loss\n",
    "class GenerateResNetFCC:\n",
    "    #Note: This is a quick dry run on the task of changing Lidar to image.\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape  # Shape of the LiDAR input (e.g., [N, 7])\n",
    "        self.output_shape = output_shape  # Target image shape (e.g., [1550, 2048, 3])\n",
    "\n",
    "        # Create the ResNet-Fully\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        #tf.print(\"bulding model\")\n",
    "        # The input layer for LiDAR data (x, y, z + flag)\n",
    "        input_layer = Input(shape=self.input_shape)\n",
    "\n",
    "        \n",
    "        x = Dense(64, activation='relu')(input_layer)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        # Projection into a larger feature space\n",
    "        x = Dense(self.output_shape, activation='relu')(x)\n",
    "\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs=50, batch_size=32, validation_split=0.2):\n",
    "        #tf.print('starting training')\n",
    "        # Train the model using the input LiDAR data and target camera images\n",
    "        history = self.model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        return history\n",
    "    def fit(self, X_train, Y_train, epochs=50, batch_size=32, validation_split=0.2)  :\n",
    "        history = self.model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "        return history\n",
    "\n",
    "    def generate_image(self, lidar_input):\n",
    "        # Use the model to generate an image given LiDAR input\n",
    "        generated_image = self.model.predict(lidar_input)\n",
    "        return generated_image\n",
    "\n",
    "    def save_model(self, path='generate_resnet_fcc_model.h5'):\n",
    "        # Save the trained model\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_model(self, path='generate_resnet_fcc_model.h5'):\n",
    "        # Load the model from a saved file\n",
    "        self.model = tf.keras.models.load_model(path)\n",
    "\n",
    "def plot_training_history(history):\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    if 'accuracy' in history.history:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train,X_test,Y_test=generate_training_data()\n",
    "X_train_flattened=flatten_x_train(X_train)\n",
    "Y_train_flattened=flatten_y_train(Y_train)\n",
    "Y_test_flattened=flatten_y_train(Y_test)\n",
    "X_test_flattened=flatten_x_train(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "  history = model.model.fit(X_train_flattened, Y_train_flattened, epochs=1000, batch_size=32, validation_data=(X_test_flattened,Y_test_flattened))\n",
    "\n",
    "plot_training_history(history)\n",
    "predicted_output = model.model.predict(X_test_flattened[0:32])\n",
    "print(predicted_output.shape)\n",
    "\n",
    "predicted_image = predicted_output.reshape(32, 1550, 2048, 3)\n",
    "predicted_image = np.clip(predicted_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "print(predicted_image[0][0][0])\n",
    "\n",
    "for i in range(32):\n",
    "    plt.imshow(predicted_image[i])\n",
    "    plt.title(\"Generated Image from Lidar\")\n",
    "    plt.axis('off')  # To hide axes\n",
    "    plt.show()\n",
    "    plt.imshow(Y_test[i])\n",
    "    plt.title(\"actual Image from camera\")\n",
    "    plt.axis('off')  # To hide axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To visualize model generated image given lidar cloud points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "predicted_output = model.model.predict(X_train_flattened[0:32])\n",
    "print(predicted_output.shape)\n",
    "predicted_image = predicted_output.reshape(32, 1550, 2048, 3)\n",
    "\n",
    "predicted_image = np.clip(predicted_image, 0, 255).astype(np.uint8)\n",
    "print(predicted_image[0][0][0])\n",
    "for i in range(32):\n",
    "    plt.imshow(predicted_image[i])\n",
    "    plt.title(\"Generated Image from Lidar\")\n",
    "    plt.axis('off')  # To hide axes\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
